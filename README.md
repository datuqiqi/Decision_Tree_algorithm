# Decision_Tree_algorithm
决策树的主要优势就在于数据形式非常容易理解

决策树的优点：计算复杂度不高，输出结果易于理解，对中间值的缺失不敏感，可以处理不相关特征数据
缺点：可能会产生过度匹配问题。
离散型和连续性数据都可以处理。

在构建决策树时，我们需要解决的第一问题就是，当前数据集上哪个特征在划分数据分类时起决定性作用。
为了找到决定性的特征，划分出最好的结果，我们必须评估每个特征。完成测试之后，原始数据集就被划
分成几个数据子集。这些数据子集会分布在第一个决策点的所有分支上，如果某个分支下的数据属于同一
类型，则不必继续分割，否则继续分割。直到所有分支无法分割。


